{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin\n",
    "import random\n",
    "\n",
    "class MultiLayerPerceptron(BaseEstimator, ClassifierMixin): \n",
    "    def __init__(self, params=None):     \n",
    "        if (params == None):\n",
    "            self.inputLayer = 4                        # Input Layer\n",
    "            self.hiddenLayer = 5                       # Hidden Layer\n",
    "            self.outputLayer = 3                       # Outpuy Layer\n",
    "            self.learningRate = 0.005                  # Learning rate\n",
    "            self.max_epochs = 600                      # Epochs\n",
    "            self.iasHiddenValue = -1                   # Bias HiddenLayer\n",
    "            self.BiasOutputValue = -1                  # Bias OutputLayer\n",
    "            self.activation = self.ativacao['sigmoid'] # Activation function\n",
    "            self.deriv = self.derivada['sigmoid']\n",
    "        else:\n",
    "            self.inputLayer = params['InputLayer']\n",
    "            self.hiddenLayer = params['HiddenLayer']\n",
    "            self.OutputLayer = params['OutputLayer']\n",
    "            self.learningRate = params['LearningRate']\n",
    "            self.max_epochs = params['Epocas']\n",
    "            self.BiasHiddenValue = params['BiasHiddenValue']\n",
    "            self.BiasOutputValue = params['BiasOutputValue']\n",
    "            self.activation = self.ativacao[params['ActivationFunction']]\n",
    "            self.deriv = self.derivada[params['ActivationFunction']]\n",
    "        \n",
    "        'Starting Bias and Weights'\n",
    "        self.WEIGHT_hidden = self.starting_weights(self.hiddenLayer, self.inputLayer)\n",
    "        self.WEIGHT_output = self.starting_weights(self.OutputLayer, self.hiddenLayer)\n",
    "        self.BIAS_hidden = np.array([self.BiasHiddenValue for i in range(self.hiddenLayer)])\n",
    "        self.BIAS_output = np.array([self.BiasOutputValue for i in range(self.OutputLayer)])\n",
    "        self.classes_number = 3 \n",
    "        \n",
    "    pass\n",
    "    \n",
    "    def starting_weights(self, x, y):\n",
    "        return [[2  * random.random() - 1 for i in range(x)] for j in range(y)]\n",
    "\n",
    "    ativacao = {\n",
    "         'sigmoid': (lambda x: 1/(1 + np.exp(-x))),\n",
    "            'tanh': (lambda x: np.tanh(x)),\n",
    "            'Relu': (lambda x: x*(x > 0)),\n",
    "               }\n",
    "    derivada = {\n",
    "         'sigmoid': (lambda x: x*(1-x)),\n",
    "            'tanh': (lambda x: 1-x**2),\n",
    "            'Relu': (lambda x: 1 * (x>0))\n",
    "               }\n",
    " \n",
    "    def Backpropagation_Algorithm(self, x):\n",
    "        DELTA_output = []\n",
    "        'Stage 1 - Error: OutputLayer'\n",
    "        ERROR_output = self.output - self.OUTPUT_L2\n",
    "        DELTA_output = ((-1)*(ERROR_output) * self.deriv(self.OUTPUT_L2))\n",
    "        \n",
    "        arrayStore = []\n",
    "        'Stage 2 - Update weights OutputLayer and HiddenLayer'\n",
    "        for i in range(self.hiddenLayer):\n",
    "            for j in range(self.OutputLayer):\n",
    "                self.WEIGHT_output[i][j] -= (self.learningRate * (DELTA_output[j] * self.OUTPUT_L1[i]))\n",
    "                self.BIAS_output[j] -= (self.learningRate * DELTA_output[j])\n",
    "      \n",
    "        'Stage 3 - Error: HiddenLayer'\n",
    "        delta_hidden = np.matmul(self.WEIGHT_output, DELTA_output)* self.deriv(self.OUTPUT_L1)\n",
    " \n",
    "        'Stage 4 - Update weights HiddenLayer and InputLayer(x)'\n",
    "        for i in range(self.OutputLayer):\n",
    "            for j in range(self.hiddenLayer):\n",
    "                self.WEIGHT_hidden[i][j] -= (self.learningRate * (delta_hidden[j] * x[i]))\n",
    "                self.BIAS_hidden[j] -= (self.learningRate * delta_hidden[j])\n",
    "                \n",
    "    def show_err_graphic(self,v_erro,v_epoca):\n",
    "        plt.figure(figsize=(9,4))\n",
    "        plt.plot(v_epoca, v_erro, \"m-\",color=\"b\", marker=11)\n",
    "        plt.xlabel(\"Number of Epochs\")\n",
    "        plt.ylabel(\"Squared error (MSE) \");\n",
    "        plt.title(\"Error Minimization\")\n",
    "        plt.show()\n",
    "\n",
    "    def predict(self, X, y):\n",
    "        'Returns the predictions for every element of X'\n",
    "        my_predictions = []\n",
    "        'Forward Propagation'\n",
    "        forward = np.matmul(X,self.WEIGHT_hidden) + self.BIAS_hidden\n",
    "        forward = np.matmul(forward, self.WEIGHT_output) + self.BIAS_output\n",
    "                                 \n",
    "        for i in forward:\n",
    "            my_predictions.append(max(enumerate(i), key=lambda x:x[1])[0])\n",
    "            \n",
    "        array_score = []\n",
    "        for i in range(len(my_predictions)):\n",
    "            if my_predictions[i] == 0: \n",
    "                array_score.append([i, 'Iris-setosa', my_predictions[i], y[i]])\n",
    "            elif my_predictions[i] == 1:\n",
    "                 array_score.append([i, 'Iris-versicolour', my_predictions[i], y[i]])\n",
    "            elif my_predictions[i] == 2:\n",
    "                 array_score.append([i, 'Iris-virginica', my_predictions[i], y[i]])\n",
    "                    \n",
    "        dataframe = pd.DataFrame(array_score, columns=['_id', 'class', 'output', 'hoped_output'])\n",
    "        return my_predictions, dataframe\n",
    "\n",
    "    def fit(self, X, y):  \n",
    "        count_epoch = 1\n",
    "        total_error = 0\n",
    "        n = len(X); \n",
    "        epoch_array = []\n",
    "        error_array = []\n",
    "        W0 = []\n",
    "        W1 = []\n",
    "        while(count_epoch <= self.max_epochs):\n",
    "            for idx,inputs in enumerate(X): \n",
    "                self.output = np.zeros(self.classes_number)\n",
    "                'Stage 1 - (Forward Propagation)'\n",
    "                self.OUTPUT_L1 = self.activation((np.dot(inputs, self.WEIGHT_hidden) + self.BIAS_hidden.T))\n",
    "                self.OUTPUT_L2 = self.activation((np.dot(self.OUTPUT_L1, self.WEIGHT_output) + self.BIAS_output.T))\n",
    "                'Stage 2 - One-Hot-Encoding'\n",
    "                if(y[idx] == 0): \n",
    "                    self.output = np.array([1,0,0]) #Class1 {1,0,0}\n",
    "                elif(y[idx] == 1):\n",
    "                    self.output = np.array([0,1,0]) #Class2 {0,1,0}\n",
    "                elif(y[idx] == 2):\n",
    "                    self.output = np.array([0,0,1]) #Class3 {0,0,1}\n",
    "                \n",
    "                square_error = 0\n",
    "                for i in range(self.OutputLayer):\n",
    "                    erro = (self.output[i] - self.OUTPUT_L2[i])**2\n",
    "                    square_error = (square_error + (0.05 * erro))\n",
    "                    total_error = total_error + square_error\n",
    "         \n",
    "                'Backpropagation : Update Weights'\n",
    "                self.Backpropagation_Algorithm(inputs)\n",
    "                \n",
    "            total_error = (total_error / n)\n",
    "            if((count_epoch % 50 == 0)or(count_epoch == 1)):\n",
    "                print(\"Epoch \", count_epoch, \"- Total Error: \",total_error)\n",
    "                error_array.append(total_error)\n",
    "                epoch_array.append(count_epoch)\n",
    "                \n",
    "            W0.append(self.WEIGHT_hidden)\n",
    "            W1.append(self.WEIGHT_output)\n",
    "             \n",
    "                \n",
    "            count_epoch += 1\n",
    "        self.show_err_graphic(error_array,epoch_array)\n",
    "        \n",
    "        plt.plot(W0[0])\n",
    "        plt.title('Weight Hidden update during training')\n",
    "        plt.legend(['neuron1', 'neuron2', 'neuron3', 'neuron4', 'neuron5'])\n",
    "        plt.ylabel('Value Weight')\n",
    "        plt.show()\n",
    "        \n",
    "        plt.plot(W1[0])\n",
    "        plt.title('Weight Output update during training')\n",
    "        plt.legend(['neuron1', 'neuron2', 'neuron3'])\n",
    "        plt.ylabel('Value Weight')\n",
    "        plt.show()\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {'InputLayer':4, 'HiddenLayer':5, 'OutputLayer':3,\n",
    "              'Epocas':700, 'LearningRate':0.005,'BiasHiddenValue':-1, \n",
    "              'BiasOutputValue':-1, 'ActivationFunction':'sigmoid'}\n",
    "\n",
    "Perceptron = MultiLayerPerceptron(dictionary)\n",
    "Perceptron.fit(train_X,train_y)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4348dbd6ce335369ad8e04655717f93bf5d2a1e307c6c0f121e97cee0eea5ebd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "pJsNjLrVhQOl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "from mlxtend.data import loadlocal_mnist\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "KqSzbcgDhQOo"
      },
      "outputs": [],
      "source": [
        "train_images,train_labels=loadlocal_mnist(images_path='dataset/train-images.idx3-ubyte', labels_path='dataset/train-labels.idx1-ubyte')\n",
        "test_images,test_labels=loadlocal_mnist(images_path='dataset/t10k-images.idx3-ubyte', labels_path='dataset/t10k-labels.idx1-ubyte')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "sFgDMrTyhQOo"
      },
      "outputs": [],
      "source": [
        "# train_images,train_labels=loadlocal_mnist(images_path='D:/Downloads/MNIST/raw/train-images-idx3-ubyte', labels_path='D:/Downloads/MNIST/raw/train-labels-idx1-ubyte')\n",
        "# test_images,test_labels=loadlocal_mnist(images_path='D:/Downloads/MNIST/raw/t10k-images-idx3-ubyte', labels_path='D:/Downloads/MNIST/raw/t10k-labels-idx1-ubyte')\n",
        "\n",
        "# np.save('train_images',train_images)\n",
        "# np.save('train_labels',train_labels)\n",
        "# np.save('test_images',test_images)\n",
        "# np.save('test_labels',test_labels)\n",
        "\n",
        "# train_df=train_df.to_numpy()\n",
        "# test_df=test_df.to_numpy()\n",
        "# train_images = train_df[:, 1:]/255\n",
        "# test_images = test_df[:, 1:]/255\n",
        "# train_labels = train_df[:, 0]\n",
        "# test_labels = test_df[:, 0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "XWd6aZjKhQOp"
      },
      "outputs": [],
      "source": [
        "\n",
        "#pre processing\n",
        "standardscalar = StandardScaler()\n",
        "\n",
        "train_images = standardscalar.fit_transform(train_images)\n",
        "test_images = standardscalar.transform(test_images)\n",
        "\n",
        "data=[]\n",
        "labels=[]\n",
        "for i in range(len(train_images)):\n",
        "    data.append(train_images[i])\n",
        "    labels.append(train_labels[i])\n",
        "for i in range(len(test_images)):\n",
        "    data.append(test_images[i])\n",
        "    labels.append(test_labels[i])\n",
        "\n",
        "data=np.array(data)\n",
        "labels=np.array(labels)\n",
        "\n",
        "#random shuffling\n",
        "data,labels=shuffle(data,labels)\n",
        "\n",
        "#dividing by max value to convert every pixel to 0-1\n",
        "data=data/255\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "xnR0e-LmhQOq",
        "outputId": "5d5077fe-ed4c-4602-afe4-5db9d34c19af"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(70000, 784)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "FhQbccD_hQOq"
      },
      "outputs": [],
      "source": [
        "def train_val_test_split(data, labels, train, val):\n",
        "    \"\"\" a function that will get dataset and training dataset fraction as input and return x_train, x_test, y_train, y_test \"\"\"\n",
        "    \n",
        "    print(\"Total length is \"+str(len(data)))\n",
        "    \n",
        "    train_set = len(data)*train//(train+val)\n",
        "    val_set = len(data)*val//(train+val)\n",
        "    \n",
        "    X_train = data[:train_set]\n",
        "    Y_train = labels[:train_set]\n",
        "    \n",
        "    X_val = data[train_set+1:train_set+val_set+1]\n",
        "    Y_val = labels[train_set+1:train_set+val_set+1]\n",
        "    \n",
        "    return X_train, Y_train, X_val, Y_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "bZ0_RqophQOr",
        "outputId": "c8899131-0472-4d01-9e41-03d67a0de6f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total length is 70000\n"
          ]
        }
      ],
      "source": [
        "x_train, y_train, x_val, y_val = train_val_test_split(data, labels, 8, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "m_SqV_ulhQOs"
      },
      "outputs": [],
      "source": [
        "class MyNeuralNetwork():\n",
        "  activations = ['relu','leaky_relu', 'sigmoid', 'linear', 'tanh', 'softmax']\n",
        "  weight_initializations = ['zero', 'random', 'normal']\n",
        "  \n",
        "  def __init__(self, n_layers, layer_sizes, activation, weight_initialization, batch_size, iter=50, lr=0.01):\n",
        "      self.n_layers = n_layers\n",
        "      self.layer_sizes = layer_sizes\n",
        "      self.activation = activation\n",
        "      self.lr = lr\n",
        "      self.weight_initialization = weight_initialization\n",
        "      self.batch_size = batch_size\n",
        "      self.iter = iter\n",
        "      self.W, self.B = self.initialize_weights()  #Weight and Bias dictionary respectively\n",
        "      self.A={}   #Activation values\n",
        "      self.Z={}   #Pre activation values\n",
        "      self.W_grad, self.B_grad = self.initialize_weights()\n",
        "      self.A_grad={}\n",
        "      self.Z_grad={}\n",
        "      self.train_loss=[]\n",
        "      self.val_loss=[]\n",
        "      \n",
        "  def activate(self, arr, name, back_prop=False):\n",
        "    if name=='relu':\n",
        "      arr[arr<0]=0\n",
        "      if back_prop:\n",
        "        arr[arr>0]=1\n",
        "            \n",
        "    elif name=='sigmoid':\n",
        "      arr = 1 / ( 1 + np.exp(-arr))\n",
        "      if back_prop :\n",
        "        arr = arr * (1 - arr)\n",
        "\n",
        "    elif name=='leaky_relu':\n",
        "      if not back_prop:\n",
        "        arr = [i if i>=0 else i*0.01 for i in arr]\n",
        "      else:\n",
        "        arr[arr<0]=0.01\n",
        "\n",
        "    elif name=='linear':\n",
        "      if back_prop:\n",
        "        arr = np.ones(arr.shape)\n",
        "\n",
        "    elif name=='tanh':\n",
        "      arr = np.tanh(arr)\n",
        "      if back_prop:\n",
        "        arr = 1 - arr**2\n",
        "    else:\n",
        "      y = np.exp(arr)\n",
        "      arr = y / np.sum(y, axis=1, keepdims=True)\n",
        "\n",
        "    arr[arr<10**(-20)]=0\n",
        "    arr[arr>1**(20)]=1**(20)\n",
        "    return arr\n",
        "\n",
        "  def w_initialize(self, name, shape):\n",
        "    x = np.zeros(shape)\n",
        "    if name == 'normal init':\n",
        "      x = np.random.normal(shape) * 0.01\n",
        "    elif name == 'random init':\n",
        "      x = np.random.rand(shape[0], shape[1]) * 0.01\n",
        "\n",
        "    return x\n",
        "\n",
        "  def initialize_weights(self):\n",
        "\n",
        "    weight = [0]\n",
        "    bias = [0]\n",
        "\n",
        "    for i in range(self.n_layers-1):\n",
        "      weight.append(self.w_initialize(self.weight_initialization, (self.layer_sizes[i], self.layer_sizes[i+1])))\n",
        "      bias.append(self.w_initialize(self.weight_initialization, (1, self.layer_sizes[i+1])))   \n",
        "    \n",
        "    return weight, bias\n",
        "\n",
        "  def fit(self, x_train, y_train, x_val, y_val):\n",
        "\n",
        "    validation_labels = np.zeros((len(x_val), max(y_val) + 1))\n",
        "    for i in range(len(x_val)):\n",
        "        validation_labels[i, y_val[i]] = 1\n",
        "            \n",
        "    for e in range(self.iter):\n",
        "\n",
        "      loss, current_batch_index, times, validation_loss=0, 0, len(x_train) // self.batch_size, 0\n",
        "          \n",
        "      for current_batch_index in range(0, len(x_train), self.batch_size):\n",
        "          \n",
        "        curr_batch = x_train[current_batch_index: min(len(x_train), current_batch_index + self.batch_size)]\n",
        "        curr_labels = y_train[current_batch_index: min(len(y_train), current_batch_index + self.batch_size)]\n",
        "        \n",
        "        #forward prop\n",
        "        self.A[0] = curr_batch\n",
        "        self.forward_propagation()\n",
        "        \n",
        "        #loss calc\n",
        "        labels = np.zeros((len(curr_labels), max(y_train) + 1))\n",
        "        for i in range(len(curr_labels)):\n",
        "            labels[i][curr_labels[i]] = 1\n",
        "        \n",
        "        loss += self.cross_entropy_loss(self.A[self.n_layers - 1], labels)\n",
        "        \n",
        "        output = self.A[self.n_layers - 1]\n",
        "        temp = output * (1 - output)\n",
        "        self.A_grad[self.n_layers - 1] = (output - labels) / temp\n",
        "        \n",
        "        #backward pass for all layers\n",
        "        self.backward_propagation()\n",
        "            \n",
        "        #weight update for all layers\n",
        "        for l in range(1, self.n_layers):\n",
        "          self.W[l] = self.W[l] - self.lr * self.W_grad[l]\n",
        "          self.B[l] = self.B[l] - self.lr * self.B_grad[l]\n",
        "            \n",
        "            \n",
        "        Y_pred = self.predict_proba(x_val)\n",
        "        validation_loss = self.cross_entropy_loss(Y_pred, validation_labels)\n",
        "\n",
        "      print(\"Training Loss of epoch \" + str(e) + \" is \" + str(loss / times))\n",
        "      print(\"Validation Loss of epoch \" + str(e)+\" is \" + str(validation_loss))\n",
        "\n",
        "      self.train_loss.append(loss / times)\n",
        "      self.val_loss.append(validation_loss)\n",
        "    \n",
        "    self.plot_training_val()\n",
        "      \n",
        "  def plot_training_val(self):\n",
        "    plt.plot(self.train_loss, 'b', label='training')\n",
        "    plt.plot(self.val_loss, 'r', label='validation')\n",
        "    plt.title(\"Loss vs epochs for \" + self.activation + \" for learning rate \" + str(self.lr))\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "      \n",
        "  def forward_propagation(self):\n",
        "\n",
        "    for i in range(1, self.n_layers):\n",
        "      self.Z[i] = np.dot(self.A[i-1], self.W[i])\n",
        "      if i != 1:\n",
        "          self.Z[i] += self.B[i]\n",
        "\n",
        "      self.A[i] = self.activate(self.Z[i], self.activation)\n",
        "\n",
        "    self.A[self.n_layers - 1] = self.activate(self.A[i], 'softmax')\n",
        "      \n",
        "  def backward_propagation(self):\n",
        "\n",
        "    for i in range(self.n_layers-1, 0, -1):\n",
        "      self.Z_grad[i] = self.activate(self.Z[i], self.activation, back_prop=True) * self.A_grad[i]\n",
        "      self.W_grad[i] = np.dot(self.A[i-1].T, self.Z_grad[i]) / len(self.A_grad[i])\n",
        "      self.B_grad[i] = np.sum(self.Z_grad[i], axis=0) / len(self.A_grad[i])\n",
        "      self.B_grad[i] = self.B_grad[i].reshape(self.B[i].shape)\n",
        "      self.A_grad[i-1] = np.dot(self.Z_grad[i], self.W[i].T)\n",
        "      \n",
        "  def predict_proba(self, X):\n",
        "    preact={}\n",
        "    act={}\n",
        "    act[0]=X\n",
        "    \n",
        "    for l in range(1, self.n_layers):\n",
        "        preact[l]=np.dot(act[l-1],self.W[l])\n",
        "        if l!=1:\n",
        "            preact[l]+=self.B[l]\n",
        "            \n",
        "        act[l]=self.activate(preact[l],self.activation)\n",
        "        \n",
        "    ans = self.activate(act[self.n_layers-1],'softmax')\n",
        "    return ans\n",
        "  \n",
        "  def predict(self, X):\n",
        "    y=self.predict_proba(X)\n",
        "\n",
        "    # return the numpy array y which contains the predicted values\n",
        "    return y.argmax(axis=1)\n",
        "  \n",
        "  def cross_entropy_loss(self, A, y):\n",
        "    #check for problems\n",
        "\n",
        "    n = len(y)\n",
        "    logp = - np.log(A[np.arange(n), y.argmax(axis=1)]+1e-10)\n",
        "    loss = np.sum(logp+1e-10)/n\n",
        "    return loss\n",
        "  \n",
        "  def score(self, X , y_true):\n",
        "    y_pred=self.predict(X)\n",
        "    \n",
        "    return np.sum(y_true==y_pred)/len(y_true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "rx3bl5A7hQOu",
        "outputId": "98bc7499-3057-40b5-913a-427e7826b12b"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "shapes (256,784) and (256,128) not aligned: 784 (dim 1) != 256 (dim 0)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_160\\2389044868.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMyNeuralNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'normal'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_160\\1420675331.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x_train, y_train, x_val, y_val)\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;31m#forward prop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurr_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;31m#loss calc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_160\\1420675331.py\u001b[0m in \u001b[0;36mforward_propagation\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: shapes (256,784) and (256,128) not aligned: 784 (dim 1) != 256 (dim 0)"
          ]
        }
      ],
      "source": [
        "net=MyNeuralNetwork(4, [256, 128, 64, 32], 'relu', 'normal', 256)\n",
        "net.fit(x_train, y_train, x_val, y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85J2gwWxhQOv"
      },
      "outputs": [],
      "source": [
        "net.fit(train_images, train_labels, test_images, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HpcQ8DehQOv",
        "outputId": "3ebff8b3-7311-4ed8-9ee8-ee8bdb9f046d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pred\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.1135"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net.score(test_images,test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1W9cjVz7hQOv"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8sFT9zRhQOw"
      },
      "outputs": [],
      "source": [
        "clf = MLPClassifier(hidden_layer_sizes=(256,128,64),random_state=1, max_iter=5, verbose=True).fit(train_images, train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEfmmMCehQOw",
        "outputId": "ed33ff0e-1373-4746-ad63-3d904f760775"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'softmax'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clf.out_activation_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoLEGnUJhQOw",
        "outputId": "080113b5-d1c6-4930-f603-e4aff45633e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9786"
            ]
          },
          "execution_count": 148,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clf.score(test_images,test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLtzUZSRhQOw",
        "outputId": "a2cedc0e-0636-4ba9-b7e8-6adbea1e0fa0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'activation': 'relu',\n",
              " 'alpha': 0.0001,\n",
              " 'batch_size': 'auto',\n",
              " 'beta_1': 0.9,\n",
              " 'beta_2': 0.999,\n",
              " 'early_stopping': False,\n",
              " 'epsilon': 1e-08,\n",
              " 'hidden_layer_sizes': (256, 128, 64),\n",
              " 'learning_rate': 'constant',\n",
              " 'learning_rate_init': 0.001,\n",
              " 'max_fun': 15000,\n",
              " 'max_iter': 5,\n",
              " 'momentum': 0.9,\n",
              " 'n_iter_no_change': 10,\n",
              " 'nesterovs_momentum': True,\n",
              " 'power_t': 0.5,\n",
              " 'random_state': 1,\n",
              " 'shuffle': True,\n",
              " 'solver': 'adam',\n",
              " 'tol': 0.0001,\n",
              " 'validation_fraction': 0.1,\n",
              " 'verbose': True,\n",
              " 'warm_start': False}"
            ]
          },
          "execution_count": 149,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clf.get_params()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

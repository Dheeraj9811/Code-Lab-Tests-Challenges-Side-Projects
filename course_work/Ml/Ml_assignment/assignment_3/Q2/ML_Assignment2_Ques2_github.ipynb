{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from mlxtend.data import loadlocal_mnist\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images,train_labels=loadlocal_mnist(images_path='dataset/train-images.idx3-ubyte', labels_path='dataset/train-labels.idx1-ubyte')\n",
    "test_images,test_labels=loadlocal_mnist(images_path='dataset/t10k-images.idx3-ubyte', labels_path='dataset/t10k-labels.idx1-ubyte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_images,train_labels=loadlocal_mnist(images_path='D:/Downloads/MNIST/raw/train-images-idx3-ubyte', labels_path='D:/Downloads/MNIST/raw/train-labels-idx1-ubyte')\n",
    "# test_images,test_labels=loadlocal_mnist(images_path='D:/Downloads/MNIST/raw/t10k-images-idx3-ubyte', labels_path='D:/Downloads/MNIST/raw/t10k-labels-idx1-ubyte')\n",
    "\n",
    "# np.save('train_images',train_images)\n",
    "# np.save('train_labels',train_labels)\n",
    "# np.save('test_images',test_images)\n",
    "# np.save('test_labels',test_labels)\n",
    "\n",
    "# train_df=train_df.to_numpy()\n",
    "# test_df=test_df.to_numpy()\n",
    "# train_images = train_df[:, 1:]/255\n",
    "# test_images = test_df[:, 1:]/255\n",
    "# train_labels = train_df[:, 0]\n",
    "# test_labels = test_df[:, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#pre processing\n",
    "standardscalar = StandardScaler()\n",
    "\n",
    "train_images = standardscalar.fit_transform(train_images)\n",
    "test_images = standardscalar.transform(test_images)\n",
    "\n",
    "data=[]\n",
    "labels=[]\n",
    "for i in range(len(train_images)):\n",
    "    data.append(train_images[i])\n",
    "    labels.append(train_labels[i])\n",
    "for i in range(len(test_images)):\n",
    "    data.append(test_images[i])\n",
    "    labels.append(test_labels[i])\n",
    "\n",
    "data=np.array(data)\n",
    "labels=np.array(labels)\n",
    "\n",
    "#random shuffling\n",
    "data,labels=shuffle(data,labels)\n",
    "\n",
    "#dividing by max value to convert every pixel to 0-1\n",
    "data=data/255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(data,labels, train,val,test):\n",
    "    \"\"\" a function that will get dataset and training dataset fraction as input and return x_train, x_test, y_train, y_test \"\"\"\n",
    "    \n",
    "    print(\"Total length is \"+str(len(data)))\n",
    "    \n",
    "    train_samples=len(data)*train//(train+test+val)\n",
    "    val_samples=len(data)*val//(train+test+val)\n",
    "    \n",
    "    train_data=data[:train_samples]\n",
    "    train_labels=labels[:train_samples]\n",
    "    \n",
    "    val_data=data[train_samples+1:train_samples+val_samples+1]\n",
    "    val_labels=labels[train_samples+1:train_samples+val_samples+1]\n",
    "    \n",
    "    test_data=data[train_samples+val_samples:]\n",
    "    test_labels=labels[train_samples+val_samples:]\n",
    "    \n",
    "    return train_data,train_labels,val_data,val_labels,test_data,test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length is 70000\n"
     ]
    }
   ],
   "source": [
    "x_train,y_train,x_val,y_val,x_test,y_test=train_val_test_split(data,labels,7,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNetwork():\n",
    "    activations = ['relu','leaky_relu', 'sigmoid', 'linear', 'tanh', 'softmax']\n",
    "    weight_initializations = ['zero', 'random', 'normal']\n",
    "    \n",
    "    def __init__(self, n_layers, layer_sizes, activation, lr, weight_initialization, batch_size, epochs):\n",
    "        self.n_layers = n_layers\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.activation = activation\n",
    "        self.lr = lr\n",
    "        self.weight_initialization = weight_initialization\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.W,self.B=self.initialize_weights()  #Weight and Bias dictionary respectively\n",
    "        self.A={}   #Activation values\n",
    "        self.Z={}   #Pre activation values\n",
    "        self.W_grad,self.B_grad=self.initialize_weights()\n",
    "        self.A_grad={}\n",
    "        self.Z_grad={}\n",
    "        self.train_loss=[]\n",
    "        self.train_epochs=[]\n",
    "        self.val_epochs=[]\n",
    "        self.val_loss=[]\n",
    "        \n",
    "    def activate(self, arr, name, grad=False):\n",
    "        if(name=='relu'):\n",
    "            if(not grad):\n",
    "                arr[arr<0]=0\n",
    "                \n",
    "            else:\n",
    "                arr[arr<0]=0\n",
    "                arr[arr>0]=1\n",
    "                \n",
    "        elif(name=='leaky_relu'):\n",
    "            if(not grad):\n",
    "                for i in arr:\n",
    "                    if(i<0):\n",
    "                        i=i*0.01\n",
    "            else:\n",
    "                arr[arr<0]=0.01\n",
    "                \n",
    "        elif(name=='sigmoid'):\n",
    "            if(not grad):\n",
    "                arr=1/(1+np.exp(-arr))\n",
    "                \n",
    "            else:\n",
    "                x=self.activate(arr,'sigmoid')\n",
    "                arr=x*(1-x)\n",
    "        \n",
    "        elif(name=='linear'):\n",
    "            if(grad):\n",
    "                arr=np.ones(arr.shape)\n",
    "                \n",
    "        elif(name=='tanh'):\n",
    "            if(not grad):\n",
    "                arr=np.tanh(arr)\n",
    "            \n",
    "            else:\n",
    "                y=np.tanh(arr)\n",
    "                arr=1-y*y\n",
    "                \n",
    "        else:\n",
    "            #softmax derivative is a jacobian matrix\n",
    "            y = np.exp(arr)\n",
    "            arr= y/(np.sum(y,axis = 1, keepdims = True))\n",
    "            \n",
    "        \n",
    "        mini=1e-20\n",
    "        maxi=1e20\n",
    "        arr[arr<mini]=0\n",
    "        arr[arr>maxi]=maxi\n",
    "        return arr\n",
    "\n",
    "    def initialization(self, name, shape):\n",
    "        x=np.zeros(shape)\n",
    "        if(name=='random'):\n",
    "            x=np.random.rand(shape[0],shape[1])*0.01\n",
    "            \n",
    "        elif(name=='normal'):\n",
    "            x=np.random.normal(size = shape)*0.01\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        Weight={}\n",
    "        Bias={}\n",
    "        \n",
    "        for i in range(self.n_layers-1):\n",
    "            Weight[i+1]=self.initialization(self.weight_initialization, (self.layer_sizes[i],self.layer_sizes[i+1]))\n",
    "            Bias[i+1]=self.initialization(self.weight_initialization, (1,self.layer_sizes[i+1]))    \n",
    "        \n",
    "        return Weight,Bias\n",
    "    \n",
    "    def fit(self,x_train,y_train,x_val=None,y_val=None):\n",
    "        val_flag=False\n",
    "        if(x_val is not None):\n",
    "            val_flag=True\n",
    "            val_labels_vector=np.zeros((len(x_val),10))\n",
    "            for i in range(len(x_val)):\n",
    "                val_labels_vector[i][y_val[i]]=1\n",
    "                \n",
    "        for e in range(self.epochs):\n",
    "            \n",
    "            print(\"Epoch \"+str(e))\n",
    "            curr_loss=0\n",
    "            curr_val_loss=0\n",
    "            sample_ind=0\n",
    "            self.train_epochs.append(e+1)\n",
    "            times=0\n",
    "            \n",
    "            if(x_val is not None):\n",
    "                self.val_epochs.append(e+1)\n",
    "                \n",
    "            while sample_ind<len(x_train):\n",
    "                times+=1\n",
    "                \n",
    "                curr_batch=x_train[sample_ind:min(len(x_train),sample_ind+self.batch_size)]\n",
    "                curr_labels=y_train[sample_ind:min(len(y_train),sample_ind+self.batch_size)]\n",
    "                \n",
    "                #forward prop\n",
    "                self.A[0]=curr_batch\n",
    "                self.forward_propagation()\n",
    "                \n",
    "                #loss calc\n",
    "                labels_vectorized=np.zeros((len(curr_labels),10))\n",
    "                for i in range(len(curr_labels)):\n",
    "                    labels_vectorized[i][curr_labels[i]]=1\n",
    "                \n",
    "                #loss\n",
    "                loss=self.cross_entropy_loss(self.A[self.n_layers-1],labels_vectorized)\n",
    "                curr_loss+=loss\n",
    "                \n",
    "                output=self.A[self.n_layers-1]\n",
    "                new_arr=output*(1-output)\n",
    "                self.A_grad[self.n_layers-1]=(output-labels_vectorized)/new_arr\n",
    "                \n",
    "                #backward pass for all layers\n",
    "                self.backward_propagation()\n",
    "                    \n",
    "                #weight update for all layers\n",
    "                for l in range(1,self.n_layers):\n",
    "                    self.W[l]=self.W[l]-self.lr*self.W_grad[l]\n",
    "                    self.B[l]=self.B[l]-self.lr*self.B_grad[l]\n",
    "                \n",
    "                sample_ind+=self.batch_size\n",
    "                \n",
    "            if(x_val is not None):\n",
    "                findd=self.predict_proba(x_val)\n",
    "                curr_val_loss=self.cross_entropy_loss(findd,val_labels_vector)\n",
    "\n",
    "            print(\"Training Loss of epoch \"+str(e)+\" is \"+str(curr_loss/times))\n",
    "            if(val_flag):\n",
    "                print(\"Validation Loss of epoch \"+str(e)+\" is \"+str(curr_val_loss))\n",
    "\n",
    "            self.train_loss.append(curr_loss/times)\n",
    "            if(x_val is not None):\n",
    "                self.val_loss.append(curr_val_loss)\n",
    "        \n",
    "        self.plot_training_val(val_flag)\n",
    "        \n",
    "    def plot_training_val(self,val_flag):\n",
    "        plt.plot(self.train_epochs,self.train_loss,'b',label='training')\n",
    "        plt.plot(self.val_epochs,self.val_loss,'r',label='validation')\n",
    "        plt.title(\"Loss vs epochs for \"+self.activation+\" for learning rate \"+str(self.lr))\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "            \n",
    "    def forward_propagation(self):\n",
    "\n",
    "        for l in range(1,self.n_layers):\n",
    "            self.Z[l]=np.dot(self.A[l-1],self.W[l])\n",
    "            if(l!=1):\n",
    "                self.Z[l]+=self.B[l]\n",
    "\n",
    "            self.A[l]=self.activate(self.Z[l],self.activation)\n",
    "        \n",
    "        #applying softmax on last layer as loss is cross entropy and we might end up taking log 0\n",
    "        self.A[l]=self.activate(self.A[l],'softmax')\n",
    "        \n",
    "    def backward_propagation(self):\n",
    "\n",
    "        for l in range(self.n_layers-1,0,-1):\n",
    "            self.Z_grad[l]=self.activate(self.Z[l],self.activation,grad=True)*self.A_grad[l]\n",
    "            assert(self.Z_grad[l].shape==self.Z[l].shape)\n",
    "            self.W_grad[l]=np.dot(self.A[l-1].T,self.Z_grad[l])/len(self.A_grad[l])\n",
    "            assert(self.W_grad[l].shape==self.W[l].shape)\n",
    "            self.B_grad[l]=np.sum(self.Z_grad[l],axis=0)/len(self.A_grad[l])\n",
    "            self.B_grad[l]=self.B_grad[l].reshape((self.B[l].shape))\n",
    "            assert(self.B_grad[l].shape==self.B[l].shape)\n",
    "            self.A_grad[l-1]=np.dot(self.Z_grad[l],self.W[l].T)\n",
    "            assert(self.A_grad[l-1].shape==self.A[l-1].shape)\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        print(\"pred\")\n",
    "        preact={}\n",
    "        act={}\n",
    "        act[0]=X\n",
    "        \n",
    "        for l in range(1,self.n_layers):\n",
    "            preact[l]=np.dot(act[l-1],self.W[l])\n",
    "            if(l!=1):\n",
    "                preact[l]+=self.B[l]\n",
    "                \n",
    "            act[l]=self.activate(preact[l],self.activation)\n",
    "            \n",
    "        ans=self.activate(act[self.n_layers-1],'softmax')\n",
    "        return ans\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y=self.predict_proba(X)\n",
    "\n",
    "        # return the numpy array y which contains the predicted values\n",
    "        return y.argmax(axis=1)\n",
    "    \n",
    "    def cross_entropy_loss(self, A, y):\n",
    "        #check for problems\n",
    "\n",
    "        n = len(y)\n",
    "        logp = - np.log(A[np.arange(n), y.argmax(axis=1)]+1e-10)\n",
    "        loss = np.sum(logp+1e-10)/n\n",
    "        return loss\n",
    "    \n",
    "    def score(self, X , y_true):\n",
    "        y_pred=self.predict(X)\n",
    "        \n",
    "        return np.sum(y_true==y_pred)/len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (128,784) and (256,128) not aligned: 784 (dim 1) != 256 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21300\\1811665685.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mnet\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMyNeuralNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'normal'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21300\\3106980765.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x_train, y_train, x_val, y_val)\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;31m#forward prop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m                 \u001b[1;31m#loss calc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21300\\3106980765.py\u001b[0m in \u001b[0;36mforward_propagation\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m             \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (128,784) and (256,128) not aligned: 784 (dim 1) != 256 (dim 0)"
     ]
    }
   ],
   "source": [
    "lrs=[1]\n",
    "for i in lrs:\n",
    "    net=MyNeuralNetwork(4,[256,128,64,32],'relu',0.01,'normal',128,20)\n",
    "    net.fit(train_images,train_labels,test_images,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.fit(train_images,train_labels,test_images,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1135"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.score(test_images,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(256,128,64),random_state=1, max_iter=5, verbose=True).fit(train_images, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'softmax'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.out_activation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9786"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(test_images,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu',\n",
       " 'alpha': 0.0001,\n",
       " 'batch_size': 'auto',\n",
       " 'beta_1': 0.9,\n",
       " 'beta_2': 0.999,\n",
       " 'early_stopping': False,\n",
       " 'epsilon': 1e-08,\n",
       " 'hidden_layer_sizes': (256, 128, 64),\n",
       " 'learning_rate': 'constant',\n",
       " 'learning_rate_init': 0.001,\n",
       " 'max_fun': 15000,\n",
       " 'max_iter': 5,\n",
       " 'momentum': 0.9,\n",
       " 'n_iter_no_change': 10,\n",
       " 'nesterovs_momentum': True,\n",
       " 'power_t': 0.5,\n",
       " 'random_state': 1,\n",
       " 'shuffle': True,\n",
       " 'solver': 'adam',\n",
       " 'tol': 0.0001,\n",
       " 'validation_fraction': 0.1,\n",
       " 'verbose': True,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.get_params()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
